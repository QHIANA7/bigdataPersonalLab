import os
import subprocess
import tempfile
import time
import json
import aiohttp
import asyncio
import logging
from datetime import datetime
from confluent_kafka import Consumer, KafkaError

# ì„¤ì •
BROKER = "s1:9092,s2:9092,s3:9092"
TOPIC = "topic1"
GROUP_ID = "fms-data-processor"
FETCH_INTERVAL = 30
HDFS_DIR = "/fms/raw-data/"

# ë¡œê¹…
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AsyncFMSConsumer:
    def __init__(self):
        self.consumer = Consumer({
            'bootstrap.servers': BROKER,
            'group.id': GROUP_ID,
            'auto.offset.reset': 'earliest'
        })
        self.buffer = []
        self.last_flush_time = time.time()

        # HDFS ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
        self.ensure_hdfs_directory(HDFS_DIR)

    def ensure_hdfs_directory(self, path):
        """HDFS ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±"""
        try:
            result = subprocess.run(
                ["hdfs", "dfs", "-test", "-d", path],
                check=False
            )
            if result.returncode != 0:
                # ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±
                subprocess.run(
                    ["hdfs", "dfs", "-mkdir", "-p", path],
                    check=True
                )
                logger.info(f"ğŸ“ HDFS ë””ë ‰í† ë¦¬ ìƒì„±ë¨: {path}")
            else:
                logger.info(f"âœ… HDFS ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ë¨: {path}")
        except Exception as e:
            logger.error(f"HDFS ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def write_buffer_to_hdfs(self):
        """ë²„í¼ ë‚´ìš©ì„ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ë¡œ HDFSì— ì €ì¥"""
        if not self.buffer:
            return

        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sensor-data-{timestamp_str}.jsonl"
        hdfs_path = os.path.join(HDFS_DIR, filename)

        try:
            with tempfile.NamedTemporaryFile('w', delete=False) as tmp:
                for record in self.buffer:
                    tmp.write(json.dumps(record) + '\n')
                tmp_path = tmp.name

            # HDFSë¡œ ì—…ë¡œë“œ
            subprocess.run(
                ["hdfs", "dfs", "-put", "-f", tmp_path, hdfs_path],
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            logger.info(f"ğŸ“¤ HDFS ì €ì¥ ì™„ë£Œ: {hdfs_path}")
        except subprocess.CalledProcessError as e:
            logger.error(f"HDFS ì €ì¥ ì‹¤íŒ¨: {e.stderr.strip()}")
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass
            self.buffer.clear()
            self.last_flush_time = time.time()

    def process_message(self, msg):
        try:
            raw_value = msg.value().decode('utf-8')
            logger.info(f"[RAW INPUT] {raw_value}")
            data = json.loads(raw_value)
            self.buffer.append(data)
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    async def run(self):
        logger.info("FMS Data Consumer ì‹œì‘... (HDFS ì €ì¥ ëª¨ë“œ)")
        logger.info(f"êµ¬ë… í† í”½: {TOPIC}")

        self.consumer.subscribe([TOPIC])

        try:
            while True:
                msg = self.consumer.poll(1.0)

                if msg is None:
                    continue
                elif msg.error():
                    if msg.error().code() != KafkaError._PARTITION_EOF:
                        logger.error(f"Consumer error: {msg.error()}")
                else:
                    self.process_message(msg)

                elapsed_time = max(0, FETCH_INTERVAL - (time.time() - self.last_flush_time))
                logger.info(f"ğŸ•’ ë‹¤ìŒ ì²˜ë¦¬ê¹Œì§€ {elapsed_time:.1f}ì´ˆ ë‚¨ìŒ... í˜„ì¬ ë²„í¼ ìˆ˜ {len(self.buffer)}")

                if elapsed_time <= 0:
                    self.write_buffer_to_hdfs()

                await asyncio.sleep(1)

        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        finally:
            self.write_buffer_to_hdfs()
            self.consumer.close()
            logger.info("Consumer ì¢…ë£Œ")

def main():
    consumer = AsyncFMSConsumer()
    asyncio.run(consumer.run())

if __name__ == "__main__":
    main()
import time
from datetime import datetime
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, row_number
from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, BooleanType
from prometheus_client import start_http_server, Gauge

# --- ì„¤ì • ---
PROMETHEUS_PORT = 9993
REFRESH_INTERVAL_SECONDS = 30
HDFS_BASE = "hdfs://s1:9000"

# --- í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì •ì˜ ---
METRIC_LABELS = ['device_id']
RAW_METRICS = {
    'sensor1': Gauge('fms_raw_latest_sensor1', 'Latest raw value of sensor1', METRIC_LABELS),
    'sensor2': Gauge('fms_raw_latest_sensor2', 'Latest raw value of sensor2', METRIC_LABELS),
    'sensor3': Gauge('fms_raw_latest_sensor3', 'Latest raw value of sensor3', METRIC_LABELS),
    'motor1': Gauge('fms_raw_latest_motor1', 'Latest raw value of motor1', METRIC_LABELS),
    'motor2': Gauge('fms_raw_latest_motor2', 'Latest raw value of motor2', METRIC_LABELS),
    'motor3': Gauge('fms_raw_latest_motor3', 'Latest raw value of motor3', METRIC_LABELS),
}

# --- JSON ë°ì´í„° ìŠ¤í‚¤ë§ˆ ---
JSON_SCHEMA = StructType() \
    .add("time", StringType()) \
    .add("DeviceId", IntegerType()) \
    .add("sensor1", DoubleType()) \
    .add("sensor2", DoubleType()) \
    .add("sensor3", DoubleType()) \
    .add("motor1", DoubleType()) \
    .add("motor2", DoubleType()) \
    .add("motor3", DoubleType()) \
    .add("isFail", BooleanType()) \
    .add("collected_at", StringType())

def update_raw_metrics(spark):
    now = datetime.now()
    partition_path = f"{HDFS_BASE}/fms/raw-data/year={now.year}/month={now.month}/day={now.day}/hour={now.hour}/"
    print(f"ğŸ”„ ìµœì‹  'raw' ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤... (ê²½ë¡œ: {partition_path})")

    try:
        # 1. HDFS ê²½ë¡œì˜ JSON íŒŒì¼ ì½ê¸°
        df_raw = spark.read.schema(JSON_SCHEMA).option("multiline", "true").json(partition_path)

        if df_raw.rdd.isEmpty():
            print("âœ… ì²˜ë¦¬í•  ìµœì‹  'raw' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for gauge in RAW_METRICS.values():
                gauge.clear()
            return

        # 2. ê° DeviceIdì—ì„œ ìµœì‹  ë ˆì½”ë“œ ì°¾ê¸°
        window_spec = Window.partitionBy("DeviceId").orderBy(col("time").desc())
        df_latest = df_raw.withColumn("rank", row_number().over(window_spec)) \
                          .filter(col("rank") == 1) \
                          .select("DeviceId", "sensor1", "sensor2", "sensor3", "motor1", "motor2", "motor3")

        # 3. í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        for gauge in RAW_METRICS.values():
            gauge.clear()

        for row in df_latest.collect():
            labels = {"device_id": str(row["DeviceId"])}
            for metric_name, gauge in RAW_METRICS.items():
                value = row[metric_name]
                if value is not None:
                    gauge.labels(**labels).set(float(value))

        print(f"âœ… {df_latest.count()}ê°œ 'raw' ì¥ë¹„ì˜ ìµœì‹  ë©”íŠ¸ë¦­ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        if "Path does not exist" in str(e):
            print(f"âš ï¸ HDFS ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {partition_path}. 'raw' ë°ì´í„°ê°€ ì•„ì§ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def main():
    spark = SparkSession.builder \
        .appName("FMS HDFS Raw to Prometheus") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    print(f"ğŸš€ 'raw' ë°ì´í„° í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (í¬íŠ¸: {PROMETHEUS_PORT})")
    start_http_server(PROMETHEUS_PORT)
    print(f"ğŸ”— ë©”íŠ¸ë¦­ í™•ì¸: http://<your-spark-driver-ip>:{PROMETHEUS_PORT}")

    try:
        while True:
            update_raw_metrics(spark)
            print(f"ğŸ•’ ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ {REFRESH_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            time.sleep(REFRESH_INTERVAL_SECONDS)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

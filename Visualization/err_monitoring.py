import time
from datetime import datetime
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, row_number, to_timestamp, current_timestamp, expr
from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, BooleanType
from prometheus_client import start_http_server, Gauge

# --- ì„¤ì • ---
PROMETHEUS_PORT = 9991
REFRESH_INTERVAL_SECONDS = 30
HDFS_BASE = "hdfs://s1:9000"

# --- í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì •ì˜ ---
METRIC_LABELS = ['device_id']
ERROR_METRICS = {
    'sensor1': Gauge('fms_error_count_per_hour', 'fms_error_count_per_hour', METRIC_LABELS)
}

# --- JSON ë°ì´í„° ìŠ¤í‚¤ë§ˆ ---
JSON_SCHEMA = StructType() \
    .add("time", StringType()) \
    .add("DeviceId", IntegerType()) \
    .add("sensor1", DoubleType()) \
    .add("sensor2", DoubleType()) \
    .add("sensor3", DoubleType()) \
    .add("motor1", DoubleType()) \
    .add("motor2", DoubleType()) \
    .add("motor3", DoubleType()) \
    .add("isFail", BooleanType()) \
    .add("collected_at", StringType())

def update_error_metrics(spark):
    now = datetime.now()
    partition_path = f"{HDFS_BASE}/fms/dataerr/year={now.year}/month={now.month}/day={now.day}/hour={now.hour}/"
    print(f"ğŸ”„ ìµœì‹  'error' ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤... (ê²½ë¡œ: {partition_path})")

    try:
        # 1. HDFS ê²½ë¡œì˜ JSON íŒŒì¼ ì½ê¸°
        df_err = spark.read.schema(JSON_SCHEMA).option("multiline", "true").json(partition_path)

        if df_err.rdd.isEmpty():
            print("âœ… ì²˜ë¦¬í•  ìµœì‹  'error' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for gauge in ERROR_METRICS.values():
                gauge.clear()
            return

        # 2-1. collected_at -> íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        df_err = df_err.withColumn("collected_at_ts", to_timestamp("collected_at"))

        # 2-2. ìµœê·¼ 1ì‹œê°„ + ìœ íš¨ ë°ì´í„° í•„í„°ë§
        df_err = df_err.filter(col("collected_at_ts") >= current_timestamp() - expr("INTERVAL 1 HOUR"))

        # 2-3. DeviceIdë³„ ì¹´ìš´íŠ¸
        device_counts = df_err.groupBy("DeviceId").count().collect()

        # 3. í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        for gauge in ERROR_METRICS.values():
            gauge.clear()

        for row in device_counts:
            labels = {"device_id": str(row["DeviceId"])}
            for metric_name, gauge in ERROR_METRICS.items():
                value = row["count"]
                if value is not None:
                    gauge.labels(**labels).set(value)

        print(f"âœ… {df_latest.count()}ê°œ 'error' ì¥ë¹„ì˜ ìµœì‹  ë©”íŠ¸ë¦­ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        if "Path does not exist" in str(e):
            print(f"âš ï¸ HDFS ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {partition_path}. 'error' ë°ì´í„°ê°€ ì•„ì§ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def main():
    spark = SparkSession.builder \
        .appName("FMS HDFS Error to Prometheus") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    print(f"ğŸš€ 'error' ë°ì´í„° í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (í¬íŠ¸: {PROMETHEUS_PORT})")
    start_http_server(PROMETHEUS_PORT)
    print(f"ğŸ”— ë©”íŠ¸ë¦­ í™•ì¸: http://<your-spark-driver-ip>:{PROMETHEUS_PORT}")

    try:
        while True:
            update_error_metrics(spark)
            print(f"ğŸ•’ ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ {REFRESH_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            time.sleep(REFRESH_INTERVAL_SECONDS)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
